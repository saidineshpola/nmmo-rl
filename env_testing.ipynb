{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space:  <bound method Env.action_space of <nmmo.core.env.Env object at 0x7fddbf56f100>>\n",
      "number of agents:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128]\n",
      "dict_keys(['CurrentTick', 'AgentId', 'Task', 'Tile', 'Entity', 'Inventory', 'Market', 'ActionTargets'])\n",
      "Task (4096,)\n",
      "Tile (225, 3)\n",
      "Entity (100, 31)\n",
      "Inventory (12, 16)\n",
      "Market (1024, 16)\n"
     ]
    }
   ],
   "source": [
    "import nmmo\n",
    "\n",
    "env = nmmo.Env()\n",
    "obs = env.reset()\n",
    "# print action space and aganets and obs\n",
    "\n",
    "print('action space: ',env.action_space)\n",
    "print('number of agents: ',env.agents)\n",
    "print(obs[1].keys())\n",
    "# print shape of each value for the key\n",
    "for key in obs[1].keys():\n",
    "    # print lentgh of each value for the key if it is a list\n",
    "    try:\n",
    "        print(key, (obs[1][key]).shape)\n",
    "    except:    \n",
    "        pass\n",
    "for step in range(10):\n",
    "   actions = {a: env.action_space(a).sample() for a in env.agents}\n",
    "   obs, rewards, dones, infos = env.step(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_np_random\n",
      "_np_seed\n",
      "_reset_required\n",
      "config\n",
      "realm:  dict_keys(['config', '_np_random', 'datastore', 'tick', 'exchange', 'map', 'log_helper', 'event_log', 'players', 'npcs', 'items', '_replay_helper'])\n",
      "obs\n",
      "_dummy_obs\n",
      "possible_agents\n",
      "_agents\n",
      "_dead_agents\n",
      "_dead_this_tick\n",
      "scripted_agents\n",
      "_gamestate_generator\n",
      "game_state\n",
      "tasks\n",
      "agent_task_map\n",
      "_dummy_task_embedding\n",
      "curriculum_file_path\n",
      "_atn_space\n",
      "_obs_space\n",
      "_str_atn_map\n"
     ]
    }
   ],
   "source": [
    "for key in env.__dict__.keys():\n",
    "    if key == 'realm':\n",
    "        print(key+': ', env.__dict__[key].__dict__.keys())\n",
    "    else:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in env.realm.players.items():\n",
    "    print(key, value.__dict__.keys())\n",
    "    for key2, value2 in value.__dict__.items():\n",
    "        print(key2, value2)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 12, 81])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming x is your tensor\n",
    "x = torch.rand(768, 16, 9, 9)\n",
    "\n",
    "# Reshape the tensor to flatten the spatial dimensions\n",
    "x = x.view(768, 16, -1)\n",
    "#print(x.shape)  # Should print torch.Size([768, 4, 81])\n",
    "from reinforcement_learning.model_util import TransformerBlock\n",
    "# RuntimeError: shape '[768, -1, 3, 4, 20]' is invalid for input of size 2985984\n",
    "l = TransformerBlock(dim=81)\n",
    "x= l(x)\n",
    "print(x.shape)  # Should print torch.Size([768, 81, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Policy values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy(\n",
      "  (policy): Baseline(\n",
      "    (tile_encoder): TileEncoder(\n",
      "      (embedding): Embedding(768, 32)\n",
      "      (tile_conv_1): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (tile_conv_2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (tile_fc): Linear(in_features=968, out_features=256, bias=True)\n",
      "    )\n",
      "    (player_encoder): PlayerEncoder(\n",
      "      (embedding): Embedding(7936, 32)\n",
      "      (agent_fc): Linear(in_features=992, out_features=256, bias=True)\n",
      "      (my_agent_fc): Linear(in_features=992, out_features=256, bias=True)\n",
      "    )\n",
      "    (item_encoder): ItemEncoder(\n",
      "      (embedding): Embedding(256, 32)\n",
      "      (fc): Linear(in_features=76, out_features=256, bias=True)\n",
      "    )\n",
      "    (inventory_encoder): InventoryEncoder(\n",
      "      (fc): Linear(in_features=3072, out_features=256, bias=True)\n",
      "    )\n",
      "    (market_encoder): MarketEncoder(\n",
      "      (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (task_encoder): TaskEncoder(\n",
      "      (fc): Linear(in_features=4096, out_features=256, bias=True)\n",
      "    )\n",
      "    (proj_fc): Linear(in_features=1280, out_features=256, bias=True)\n",
      "    (action_decoder): ActionDecoder(\n",
      "      (layers): ModuleDict(\n",
      "        (attack_style): Linear(in_features=256, out_features=3, bias=True)\n",
      "        (attack_target): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (market_buy): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (inventory_destroy): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (inventory_give_item): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (inventory_give_player): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (gold_quantity): Linear(in_features=256, out_features=99, bias=True)\n",
      "        (gold_target): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (move): Linear(in_features=256, out_features=5, bias=True)\n",
      "        (inventory_sell): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (inventory_price): Linear(in_features=256, out_features=99, bias=True)\n",
      "        (inventory_use): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (value_head): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# I want to see what is inside the pytorch model/home/saidinesh/Desktop/Projects/baselines/starter-kit-nmmo/my-submission/nmmo_20231108_125354.000305.pt\n",
    "import torch\n",
    "model = torch.load('/home/saidinesh/Desktop/Projects/baselines/starter-kit-nmmo/my-submission/nmmo_20231108_125354.000305.pt')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['datastore_record', 'id', 'npc_type', 'row', 'col', 'damage', 'time_alive', 'freeze', 'item_level', 'attacker_id', 'latest_combat_tick', 'message', 'gold', 'health', 'food', 'water', 'melee_level', 'melee_exp', 'range_level', 'range_exp', 'mage_level', 'mage_exp', 'fishing_level', 'fishing_exp', 'herbalism_level', 'herbalism_exp', 'prospecting_level', 'prospecting_exp', 'carving_level', 'carving_exp', 'alchemy_level', 'alchemy_exp', 'realm', 'config', '_np_random', 'policy', 'entity_id', 'repr', 'name', 'vision', 'attacker', 'target', 'closest', 'spawn_pos', 'status', 'history', 'resources', 'inventory', 'agent', 'immortal', 'buys', 'sells', 'ration_consumed', 'poultice_consumed', 'ration_level_consumed', 'poultice_level_consumed', 'skills'])\n"
     ]
    }
   ],
   "source": [
    "from nmmo.lib.log import EventCode\n",
    "\n",
    "#env = nmmo.Env()\n",
    "log = env.realm.event_log.get_data( #agents=[1],\n",
    "                                        event_code=EventCode.PLAYER_KILL,\n",
    "                                        tick=env.realm.tick)\n",
    "\n",
    "# print  env.realm.players attri \n",
    "x= env.realm.players[1].equipment\n",
    "x=x.__dict__.keys()\n",
    "print(env.realm.players[1].__dict__.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 67.7%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def get_memory_usage():\n",
    "    memory = psutil.virtual_memory()\n",
    "    return memory.percent\n",
    "\n",
    "print(f\"Memory usage: {get_memory_usage()}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curriculum exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /home/saidinesh/Desktop/Projects/baselines/reinforcement_learning/eval_task_with_embedding.pkl\n",
    "# read that file and print keys\n",
    "import dill\n",
    "with open('/home/saidinesh/Desktop/Projects/baselines/curriculum_generation/custom_curriculum_with_embedding.pkl', 'rb') as f:\n",
    "    data = dill.load(f)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the data list and print the keys\n",
    "# each element is TaskSpec(eval_fn=<function CountEvent at 0x7fecb8f67520>, eval_fn_kwargs={'event': 'EAT_FOOD', 'N': 10}, task_cls=<class 'nmmo.task.task_api.Task'>, task_kwargs={}, reward_to='agent', sampling_weight=1.0, embedding=array([-0.3462,  0.1978, -0.2805, ..., -0.1089,  0.0671, -0.0725],\n",
    "# now create dataframe with data eval_fn_kwargs, task_cls, task_kwargs,\n",
    "import pandas as pd\n",
    "# create empty dataframe\n",
    "df = pd.DataFrame(columns=['eval_fn_kwargs',  ])\n",
    "# iterate through the data list and append the data to the dataframe\n",
    "#AttributeError: 'DataFrame' object has no attribute 'append'\n",
    "for i in range(len(data)):\n",
    "    print(data[i].eval_fn_kwargs, ) # {'num_tick': 1024} <class 'nmmo.task.task_api.Task'> \n",
    "    df = df._append({'eval_fn_kwargs': data[i].eval_fn_kwargs,  },ignore_index=True)\n",
    "df.to_csv('reinforcement_learning/curriculum_task_with_embedding.csv')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the model from the .pt file\n",
    "model = torch.load('runs/nmmo_rp_cr_attn_lstm_seed9_exp17/policy_store/nmmo_rp_cr_attn_lstm_seed9_exp17.000305.pt')\n",
    "\n",
    "# Extract the model state dictionary\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Save the state dictionary to a .pth file\n",
    "torch.save(state_dict, 'runs/nmmo_rp_cr_attn_lstm_seed9_exp17/policy_store/nmmo_rp_cr_attn_lstm_seed9_exp17.000305.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "\n",
    "def create_custom_policy_pt(policy_file, pth_file, out_name=\"my_submission.pkl\"):\n",
    "  assert out_name.endswith(\".pkl\"), \"The file name must end with .pkl\"\n",
    "  with open(policy_file, \"r\") as f:\n",
    "    src_code = f.read()\n",
    "\n",
    "  # add the make_policy() function\n",
    "  # YOU SHOULD CHECK the name of your policy (if not Baseline),\n",
    "  # and the args that go into the policy\n",
    "  src_code += \"\"\"\n",
    "\n",
    "class Config(nmmo.config.Default):\n",
    "    PROVIDE_ACTION_TARGETS = True\n",
    "    PROVIDE_NOOP_ACTION_TARGET = True\n",
    "    MAP_FORCE_GENERATION = False\n",
    "    TASK_EMBED_DIM = 4096\n",
    "    COMMUNICATION_SYSTEM_ENABLED = False\n",
    "\n",
    "\n",
    "def make_policy():\n",
    "      from pufferlib.frameworks import cleanrl\n",
    "      env = pufferlib.emulation.PettingZooPufferEnv(nmmo.Env(Config()))\n",
    "      learner_policy = BaselineNew(\n",
    "          env,\n",
    "          input_size=256,\n",
    "          hidden_size=256,\n",
    "          task_size=4096\n",
    "      )\n",
    "      return cleanrl.Policy(learner_policy)\n",
    "  \"\"\"\n",
    "  state_dict = torch.load(pth_file)\n",
    "  checkpoint = {\n",
    "    \"policy_src\": src_code,\n",
    "    \"state_dict\": state_dict,\n",
    "  }\n",
    "  with open(out_name, \"wb\") as out_file:\n",
    "    pickle.dump(checkpoint, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_policy_file='reinforcement_learning/rl_policy.py'\n",
    "checkpoint_to_submit='runs/nmmo_cr_lstm_silu_ATTACK_seed768_texp20/policy_store/nmmo_cr_lstm_silu_ATTACK_seed768_texp20.000366_state.pth'\n",
    "create_custom_policy_pt(custom_policy_file, checkpoint_to_submit,\n",
    "                        out_name=\"/home/saidinesh/Desktop/Projects/nmmo_baselines/starter-kit-nmmo/my-submission/submission_exp20_fix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['policy_src', 'state_dict'])\n"
     ]
    }
   ],
   "source": [
    "# read /home/saidinesh/Desktop/Projects/nmmo_baselines/starter-kit-nmmo/my-submission\n",
    "import pickle\n",
    "with open('/home/saidinesh/Desktop/Projects/nmmo_baselines/starter-kit-nmmo/my-submission/submission_exp20_fix.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from torch.nn import MultiheadAttention\\nimport argparse\\nimport torch\\nimport torch.nn.functional as F\\nfrom typing import Dict\\n\\nimport pufferlib\\nimport pufferlib.emulation\\nimport pufferlib.models\\n#from reinforcement_learning.model_util import PopArt\\nimport nmmo\\nfrom nmmo.entity.entity import EntityState\\n\\nEntityId = EntityState.State.attr_name_to_col[\"id\"]\\n\\n\\nclass BaselineNew(pufferlib.models.Policy):\\n    def __init__(self, env, input_size=256, hidden_size=256, task_size=4096):\\n        super().__init__(env)\\n\\n        self.flat_observation_space = env.flat_observation_space\\n        self.flat_observation_structure = env.flat_observation_structure\\n\\n        self.tile_encoder = TileEncoder(input_size)\\n        self.player_encoder = PlayerEncoder(input_size, hidden_size)\\n        self.item_encoder = ItemEncoder(input_size, hidden_size)\\n        self.inventory_encoder = InventoryEncoder(input_size, hidden_size)\\n        self.market_encoder = MarketEncoder(input_size, hidden_size)\\n        self.task_encoder = TaskEncoder(input_size, hidden_size, task_size)\\n        self.proj_fc = torch.nn.Linear(5 * input_size, input_size)\\n        self.action_decoder = ActionDecoder(input_size, hidden_size)\\n        self.value_head = torch.nn.Linear(hidden_size, 1)\\n        self.device = torch.device(\\n            \"cuda\" if torch.cuda.is_available() else \"cpu\")\\n        # self.value_head = PopArt(hidden_size, 1, device=self.device)\\n\\n    def encode_observations(self, flat_observations):\\n        env_outputs = pufferlib.emulation.unpack_batched_obs(flat_observations,\\n                                                             self.flat_observation_space, self.flat_observation_structure)\\n        tile = self.tile_encoder(env_outputs[\"Tile\"])\\n        player_embeddings, my_agent = self.player_encoder(\\n            env_outputs[\"Entity\"], env_outputs[\"AgentId\"][:, 0]\\n        )\\n\\n        item_embeddings = self.item_encoder(env_outputs[\"Inventory\"])\\n        inventory = self.inventory_encoder(item_embeddings)\\n\\n        market_embeddings = self.item_encoder(env_outputs[\"Market\"])\\n        market = self.market_encoder(market_embeddings)\\n\\n        task = self.task_encoder(env_outputs[\"Task\"])\\n\\n        obs = torch.cat([tile, my_agent, inventory, market, task], dim=-1)\\n        obs = self.proj_fc(obs)\\n\\n        return obs, (\\n            player_embeddings,\\n            item_embeddings,\\n            market_embeddings,\\n            env_outputs[\"ActionTargets\"],\\n        )\\n\\n    def decode_actions(self, hidden, lookup):\\n        actions = self.action_decoder(hidden, lookup)\\n        value = self.value_head(hidden)\\n        return actions, value\\n\\n    def forward(self, env_outputs):\\n        \\'\\'\\'Forward pass for PufferLib compatibility\\'\\'\\'\\n        hidden, lookup = self.encode_observations(env_outputs)\\n        actions, value = self.decode_actions(hidden, lookup)\\n\\n        return actions, value\\n\\n\\nclass SelfAttention(torch.nn.Module):\\n    \"\"\"\\n    Implementation of Self Attention Layer from \\n    https://arxiv.org/pdf/2305.17352.pdf\\n    \"\"\"\\n\\n    def __init__(self, input_size, heads=4, embed_size=32):\\n        super().__init__()\\n        self.input_size = input_size\\n        self.heads = heads\\n        self.emb_size = embed_size\\n\\n        self.tokeys = torch.nn.Linear(\\n            self.input_size, self.emb_size * heads, bias=False)\\n        self.toqueries = torch.nn.Linear(\\n            self.input_size, self.emb_size * heads, bias=False)\\n        self.tovalues = torch.nn.Linear(\\n            self.input_size, self.emb_size * heads, bias=False)\\n\\n    def forward(self, x):\\n        b, t, hin = x.size()\\n        assert hin == self.input_size, f\\'Input size {{hin}} should match {{self.input_size}}\\'\\n\\n        h = self.heads\\n        e = self.emb_size\\n\\n        keys = self.tokeys(x).view(b, t, h, e)\\n        queries = self.toqueries(x).view(b, t, h, e)\\n        values = self.tovalues(x).view(b, t, h, e)\\n\\n        # dot-product attention\\n        # folding heads to batch dimensions\\n        keys = keys.transpose(1, 2).contiguous().view(b * h, t, e)\\n        queries = queries.transpose(1, 2).contiguous().view(b * h, t, e)\\n        values = values.transpose(1, 2).contiguous().view(b * h, t, e)\\n\\n        queries = queries / (e ** (1/4))\\n        keys = keys / (e ** (1/4))\\n\\n        dot = torch.bmm(queries, keys.transpose(1, 2))\\n        assert dot.size() == (b*h, t, t)\\n\\n        # row wise self attention probabilities\\n        dot = F.softmax(dot, dim=2)\\n        self.dot = dot\\n        out = torch.bmm(dot, values).view(b, h, t, e)\\n        out = out.transpose(1, 2).contiguous().view(b, t, h * e)\\n        values = values.view(b, h, t, e)\\n        values = values.transpose(1, 2).contiguous().view(b, t, h * e)\\n        self.values = values\\n        return out\\n\\n\\nclass TileEncoder(torch.nn.Module):\\n    def __init__(self, input_size):\\n        super().__init__()\\n        self.tile_offset = torch.tensor([i * 256 for i in range(3)])\\n        self.embedding = torch.nn.Embedding(3 * 256, 32)\\n\\n        self.multihead_attn = MultiheadAttention(\\n            16, 4)  # Added MultiheadAttention layer\\n\\n        self.tile_conv_1 = torch.nn.Conv2d(96, 64, 3)\\n        self.tile_conv_2 = torch.nn.Conv2d(64, 32, 3)\\n        self.tile_conv_3 = torch.nn.Conv2d(32, 16, 3)\\n        self.tile_fc = torch.nn.Linear(16 * 9 * 9, input_size)\\n        self.activation = torch.nn.SiLU()\\n\\n    def forward(self, tile):\\n        tile[:, :, :2] -= tile[:, 112:113, :2].clone()\\n        tile[:, :, :2] += 7\\n        tile = self.embedding(\\n            tile.long().clip(0, 255) + self.tile_offset.to(tile.device)\\n        )\\n\\n        agents, tiles, features, embed = tile.shape\\n        tile = (\\n            tile.view(agents, tiles, features * embed)\\n            .transpose(1, 2)\\n            .view(agents, features * embed, 15, 15)\\n        )\\n\\n        tile = self.activation(self.tile_conv_1(tile))\\n        tile = self.activation(self.tile_conv_2(tile))\\n        tile = self.activation(self.tile_conv_3(tile))  # Additional layer\\n        # Reshape for MultiheadAttention\\n        tile = tile.view(tile.size(0), tile.size(1), -1).permute(2, 0, 1)\\n        tile, _ = self.multihead_attn(\\n            tile, tile, tile)  # Apply MultiheadAttention\\n        tile = tile.permute(1, 2, 0).view(agents, 16, 9, 9)  # Reshape back\\n        tile = tile.contiguous().view(agents, -1)\\n        tile = self.activation(self.tile_fc(tile))\\n\\n        return tile\\n\\n\\nclass PlayerEncoder(torch.nn.Module):\\n    def __init__(self, input_size, hidden_size):\\n        super().__init__()\\n        self.entity_dim = 31\\n        self.player_offset = torch.tensor(\\n            [i * 256 for i in range(self.entity_dim)])\\n        self.embedding = torch.nn.Embedding(self.entity_dim * 256, 32)\\n\\n        self.agent_fc = torch.nn.Linear(self.entity_dim * 32, hidden_size)\\n        self.my_agent_fc = torch.nn.Linear(self.entity_dim * 32, input_size)\\n\\n    def forward(self, agents, my_id):\\n        # Pull out rows corresponding to the agent\\n        agent_ids = agents[:, :, EntityId]\\n        mask = (agent_ids == my_id.unsqueeze(1)) & (agent_ids != 0)\\n        mask = mask.int()\\n        row_indices = torch.where(\\n            mask.any(dim=1), mask.argmax(\\n                dim=1), torch.zeros_like(mask.sum(dim=1))\\n        )\\n\\n        agent_embeddings = self.embedding(\\n            agents.long().clip(0, 255) + self.player_offset.to(agents.device)\\n        )\\n        batch, agent, attrs, embed = agent_embeddings.shape\\n\\n        # Embed each feature separately\\n        agent_embeddings = agent_embeddings.view(batch, agent, attrs * embed)\\n        my_agent_embeddings = agent_embeddings[\\n            torch.arange(agents.shape[0]), row_indices\\n        ]\\n\\n        # Project to input of recurrent size\\n        agent_embeddings = self.agent_fc(agent_embeddings)\\n        my_agent_embeddings = self.my_agent_fc(my_agent_embeddings)\\n        my_agent_embeddings = F.silu(my_agent_embeddings)\\n\\n        return agent_embeddings, my_agent_embeddings\\n\\n\\nclass ItemEncoder(torch.nn.Module):\\n    def __init__(self, input_size, hidden_size):\\n        super().__init__()\\n        self.item_offset = torch.tensor([i * 256 for i in range(16)])\\n        self.embedding = torch.nn.Embedding(256, 32)\\n\\n        self.fc = torch.nn.Linear(2 * 32 + 12, hidden_size)\\n\\n        self.discrete_idxs = [1, 14]\\n        self.discrete_offset = torch.Tensor([2, 0])\\n        self.continuous_idxs = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15]\\n        self.continuous_scale = torch.Tensor(\\n            [\\n                1 / 10,\\n                1 / 10,\\n                1 / 10,\\n                1 / 100,\\n                1 / 100,\\n                1 / 100,\\n                1 / 40,\\n                1 / 40,\\n                1 / 40,\\n                1 / 100,\\n                1 / 100,\\n                1 / 100,\\n            ]\\n        )\\n\\n    def forward(self, items):\\n        if self.discrete_offset.device != items.device:\\n            self.discrete_offset = self.discrete_offset.to(items.device)\\n            self.continuous_scale = self.continuous_scale.to(items.device)\\n\\n        # Embed each feature separately\\n        discrete = items[:, :, self.discrete_idxs] + self.discrete_offset\\n        discrete = self.embedding(discrete.long().clip(0, 255))\\n        batch, item, attrs, embed = discrete.shape\\n        discrete = discrete.view(batch, item, attrs * embed)\\n\\n        continuous = items[:, :, self.continuous_idxs] / self.continuous_scale\\n\\n        item_embeddings = torch.cat([discrete, continuous], dim=-1)\\n        item_embeddings = self.fc(item_embeddings)\\n        return item_embeddings\\n\\n\\nclass InventoryEncoder(torch.nn.Module):\\n    def __init__(self, input_size, hidden_size):\\n        super().__init__()\\n        self.fc = torch.nn.Linear(12 * hidden_size, input_size)\\n\\n    def forward(self, inventory):\\n        agents, items, hidden = inventory.shape\\n        inventory = inventory.view(agents, items * hidden)\\n        return self.fc(inventory)\\n\\n\\nclass MarketEncoder(torch.nn.Module):\\n    def __init__(self, input_size, hidden_size):\\n        super().__init__()\\n        self.fc = torch.nn.Linear(hidden_size, input_size)\\n\\n    def forward(self, market):\\n        return self.fc(market).mean(-2)\\n\\n\\nclass TaskEncoder(torch.nn.Module):\\n    def __init__(self, input_size, hidden_size, task_size):\\n        super().__init__()\\n        self.fc = torch.nn.Linear(task_size, input_size)\\n\\n    def forward(self, task):\\n        return self.fc(task.clone())\\n\\n\\nclass ActionDecoder(torch.nn.Module):\\n    def __init__(self, input_size, hidden_size):\\n        super().__init__()\\n        self.layers = torch.nn.ModuleDict(\\n            {\\n                \"attack_style\": torch.nn.Linear(hidden_size, 3),\\n                \"attack_target\": torch.nn.Linear(hidden_size, hidden_size),\\n                \"market_buy\": torch.nn.Linear(hidden_size, hidden_size),\\n                \"inventory_destroy\": torch.nn.Linear(hidden_size, hidden_size),\\n                \"inventory_give_item\": torch.nn.Linear(hidden_size, hidden_size),\\n                \"inventory_give_player\": torch.nn.Linear(hidden_size, hidden_size),\\n                \"gold_quantity\": torch.nn.Linear(hidden_size, 99),\\n                \"gold_target\": torch.nn.Linear(hidden_size, hidden_size),\\n                \"move\": torch.nn.Linear(hidden_size, 5),\\n                \"inventory_sell\": torch.nn.Linear(hidden_size, hidden_size),\\n                \"inventory_price\": torch.nn.Linear(hidden_size, 99),\\n                \"inventory_use\": torch.nn.Linear(hidden_size, hidden_size),\\n            }\\n        )\\n        self.attn = SelfAttention(hidden_size, 4, hidden_size//4)\\n        self.fc = torch.nn.Linear(hidden_size * 2, hidden_size)\\n        self.rnn = torch.nn.LSTM(\\n            input_size=hidden_size,\\n            hidden_size=hidden_size,\\n            num_layers=5,\\n            batch_first=True,\\n        )\\n        self.prev_player_states = None\\n        self.prev_inventory_states = None\\n        self.prev_hidden_states = None\\n\\n    def apply_layer(self, layer, embeddings, mask, hidden):\\n        hidden = layer(hidden)\\n        if hidden.dim() == 2 and embeddings is not None:\\n            hidden = torch.matmul(embeddings, hidden.unsqueeze(-1)).squeeze(-1)\\n\\n        if mask is not None:\\n            hidden = hidden.masked_fill(mask == 0, -1e9)\\n\\n        return hidden\\n\\n    def forward(self, hidden, lookup):\\n        (\\n            player_embeddings,\\n            inventory_embeddings,\\n            market_embeddings,\\n            action_targets,\\n        ) = lookup\\n        # player_embeddings.shape:  torch.Size([768, 100, 256])\\n        # inventory_embeddings.shape:  torch.Size([768, 12, 256])\\n        # market_embeddings.shape:  torch.Size([768, 1024, 256])\\n        # hidden.shape:  torch.Size([768, 256])\\n\\n        player_embeddings_before = player_embeddings.clone()\\n        inventory_embeddings_before = inventory_embeddings.clone()\\n        hidden_before = hidden.clone()\\n        hidden = hidden.unsqueeze(1)\\n        # Check for the prev states shape\\n        if self.prev_player_states is None or self.prev_player_states[0].shape != (self.rnn.num_layers, player_embeddings.shape[0], self.rnn.hidden_size):\\n            h_0 = torch.zeros(\\n                self.rnn.num_layers, player_embeddings.shape[0], self.rnn.hidden_size, device=player_embeddings.device)\\n            c_0 = torch.zeros(\\n                self.rnn.num_layers, player_embeddings.shape[0], self.rnn.hidden_size, device=player_embeddings.device)\\n            self.prev_player_states = (h_0, c_0)\\n        if self.prev_inventory_states is None or self.prev_inventory_states[0].shape != (self.rnn.num_layers, inventory_embeddings.shape[0], self.rnn.hidden_size):\\n            h_0 = torch.zeros(\\n                self.rnn.num_layers, inventory_embeddings.shape[0], self.rnn.hidden_size, device=inventory_embeddings.device)\\n            c_0 = torch.zeros(\\n                self.rnn.num_layers, inventory_embeddings.shape[0], self.rnn.hidden_size, device=inventory_embeddings.device)\\n            self.prev_inventory_states = (h_0, c_0)\\n        if self.prev_hidden_states is None or self.prev_hidden_states[0].shape != (self.rnn.num_layers, hidden.shape[0], self.rnn.hidden_size):\\n            h_0 = torch.zeros(\\n                self.rnn.num_layers, hidden.shape[0], self.rnn.hidden_size, device=hidden.device)\\n            c_0 = torch.zeros(\\n                self.rnn.num_layers, hidden.shape[0], self.rnn.hidden_size, device=hidden.device)\\n            self.prev_hidden_states = (h_0, c_0)\\n        player_embeddings, self.prev_player_states = self.rnn(\\n            player_embeddings, self.prev_player_states)\\n        inventory_embeddings, self.prev_inventory_states = self.rnn(\\n            inventory_embeddings, self.prev_inventory_states)\\n\\n        player_embeddings = self.attn(player_embeddings)\\n        inventory_embeddings = self.attn(inventory_embeddings)\\n        hidden, self.prev_hidden_states = self.rnn(\\n            hidden, self.prev_hidden_states)\\n        hidden = self.attn(hidden)\\n        hidden = hidden.squeeze(1)\\n        # Concat before and after attention and use MLP (advise communicaion) to get same shape as before\\n        player_embeddings = torch.cat(\\n            [player_embeddings_before, player_embeddings], dim=-1)\\n        inventory_embeddings = torch.cat(\\n            [inventory_embeddings_before, inventory_embeddings], dim=-1)\\n        hidden = torch.cat([hidden_before, hidden], dim=-1)\\n\\n        player_embeddings = torch.nn.functional.relu(\\n            self.fc(player_embeddings), inplace=True)\\n        inventory_embeddings = torch.nn.functional.relu(\\n            self.fc(inventory_embeddings), inplace=True)\\n        hidden = torch.nn.functional.relu(self.fc(hidden), inplace=True)\\n        embeddings = {\\n            \"attack_target\": player_embeddings,\\n            \"market_buy\": market_embeddings,\\n            \"inventory_destroy\": inventory_embeddings,\\n            \"inventory_give_item\": inventory_embeddings,\\n            \"inventory_give_player\": player_embeddings,\\n            \"gold_target\": player_embeddings,\\n            \"inventory_sell\": inventory_embeddings,\\n            \"inventory_use\": inventory_embeddings,\\n        }\\n\\n        action_targets = {\\n            \"attack_style\": action_targets[\"Attack\"][\"Style\"],\\n            \"attack_target\": action_targets[\"Attack\"][\"Target\"],\\n            \"market_buy\": action_targets[\"Buy\"][\"MarketItem\"],\\n            \"inventory_destroy\": action_targets[\"Destroy\"][\"InventoryItem\"],\\n            \"inventory_give_item\": action_targets[\"Give\"][\"InventoryItem\"],\\n            \"inventory_give_player\": action_targets[\"Give\"][\"Target\"],\\n            \"gold_quantity\": action_targets[\"GiveGold\"][\"Price\"],\\n            \"gold_target\": action_targets[\"GiveGold\"][\"Target\"],\\n            \"move\": action_targets[\"Move\"][\"Direction\"],\\n            \"inventory_sell\": action_targets[\"Sell\"][\"InventoryItem\"],\\n            \"inventory_price\": action_targets[\"Sell\"][\"Price\"],\\n            \"inventory_use\": action_targets[\"Use\"][\"InventoryItem\"],\\n        }\\n\\n        actions = []\\n        for key, layer in self.layers.items():\\n            mask = None\\n            mask = action_targets[key]\\n            embs = embeddings.get(key)\\n            if embs is not None and embs.shape[1] != mask.shape[1]:\\n                b, _, f = embs.shape\\n                zeros = torch.zeros(\\n                    [b, 1, f], dtype=embs.dtype, device=embs.device)\\n                embs = torch.cat([embs, zeros], dim=1)\\n\\n            action = self.apply_layer(layer, embs, mask, hidden)\\n            actions.append(action)\\n        # detach the prev states\\n        self.prev_player_states = (\\n            self.prev_player_states[0].detach(), self.prev_player_states[1].detach())\\n        self.prev_inventory_states = (\\n            self.prev_inventory_states[0].detach(), self.prev_inventory_states[1].detach())\\n        self.prev_hidden_states = (\\n            self.prev_hidden_states[0].detach(), self.prev_hidden_states[1].detach())\\n        return actions\\n\\n\\nclass Config(nmmo.config.Default):\\n    PROVIDE_ACTION_TARGETS = True\\n    PROVIDE_NOOP_ACTION_TARGET = True\\n    MAP_FORCE_GENERATION = False\\n    TASK_EMBED_DIM = 4096\\n    COMMUNICATION_SYSTEM_ENABLED = False\\n\\n\\ndef make_policy():\\n      from pufferlib.frameworks import cleanrl\\n      env = pufferlib.emulation.PettingZooPufferEnv(nmmo.Env(Config()))\\n      learner_policy = BaselineNew(\\n          env,\\n          input_size=256,\\n          hidden_size=256,\\n          task_size=4096\\n      )\\n      return cleanrl.Policy(learner_policy)\\n  '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['policy_src']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
