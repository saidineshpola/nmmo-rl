{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space:  <bound method Env.action_space of <nmmo.core.env.Env object at 0x7f854be130a0>>\n",
      "number of agents:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128]\n",
      "dict_keys(['CurrentTick', 'AgentId', 'Task', 'Tile', 'Entity', 'Inventory', 'Market', 'ActionTargets'])\n",
      "Task (4096,)\n",
      "Tile (225, 3)\n",
      "Entity (100, 31)\n",
      "Inventory (12, 16)\n",
      "Market (1024, 16)\n"
     ]
    }
   ],
   "source": [
    "import nmmo\n",
    "\n",
    "env = nmmo.Env()\n",
    "obs = env.reset()\n",
    "# print action space and aganets and obs\n",
    "\n",
    "print('action space: ',env.action_space)\n",
    "print('number of agents: ',env.agents)\n",
    "print(obs[1].keys())\n",
    "# print shape of each value for the key\n",
    "for key in obs[1].keys():\n",
    "    # print lentgh of each value for the key if it is a list\n",
    "    try:\n",
    "        print(key, (obs[1][key]).shape)\n",
    "    except:    \n",
    "        pass\n",
    "for step in range(10):\n",
    "   actions = {a: env.action_space(a).sample() for a in env.agents}\n",
    "   obs, rewards, dones, infos = env.step(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_np_random\n",
      "_np_seed\n",
      "_reset_required\n",
      "config\n",
      "realm:  dict_keys(['config', '_np_random', 'datastore', 'tick', 'exchange', 'map', 'log_helper', 'event_log', 'players', 'npcs', 'items', '_replay_helper'])\n",
      "obs\n",
      "_dummy_obs\n",
      "possible_agents\n",
      "_agents\n",
      "_dead_agents\n",
      "_dead_this_tick\n",
      "scripted_agents\n",
      "_gamestate_generator\n",
      "game_state\n",
      "tasks\n",
      "agent_task_map\n",
      "_dummy_task_embedding\n",
      "curriculum_file_path\n",
      "_atn_space\n",
      "_obs_space\n",
      "_str_atn_map\n"
     ]
    }
   ],
   "source": [
    "for key in env.__dict__.keys():\n",
    "    if key == 'realm':\n",
    "        print(key+': ', env.__dict__[key].__dict__.keys())\n",
    "    else:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 dict_keys(['datastore_record', 'id', 'npc_type', 'row', 'col', 'damage', 'time_alive', 'freeze', 'item_level', 'attacker_id', 'latest_combat_tick', 'message', 'gold', 'health', 'food', 'water', 'melee_level', 'melee_exp', 'range_level', 'range_exp', 'mage_level', 'mage_exp', 'fishing_level', 'fishing_exp', 'herbalism_level', 'herbalism_exp', 'prospecting_level', 'prospecting_exp', 'carving_level', 'carving_exp', 'alchemy_level', 'alchemy_exp', 'realm', 'config', '_np_random', 'policy', 'entity_id', 'repr', 'name', 'vision', 'attacker', 'target', 'closest', 'spawn_pos', 'status', 'history', 'resources', 'inventory', 'agent', 'immortal', 'buys', 'sells', 'ration_consumed', 'poultice_consumed', 'ration_level_consumed', 'poultice_level_consumed', 'skills'])\n",
      "datastore_record <nmmo.datastore.datastore.DatastoreRecord object at 0x7efe220a7f10>\n",
      "id <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76bc0>\n",
      "npc_type <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76bf0>\n",
      "row <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76c20>\n",
      "col <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76c50>\n",
      "damage <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76c80>\n",
      "time_alive <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76b90>\n",
      "freeze <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76cb0>\n",
      "item_level <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76ce0>\n",
      "attacker_id <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76d10>\n",
      "latest_combat_tick <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76d40>\n",
      "message <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76d70>\n",
      "gold <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76da0>\n",
      "health <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76dd0>\n",
      "food <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76e00>\n",
      "water <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76e30>\n",
      "melee_level <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76e60>\n",
      "melee_exp <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76e90>\n",
      "range_level <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76ec0>\n",
      "range_exp <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76ef0>\n",
      "mage_level <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76f20>\n",
      "mage_exp <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76f50>\n",
      "fishing_level <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76f80>\n",
      "fishing_exp <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76fb0>\n",
      "herbalism_level <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe76fe0>\n",
      "herbalism_exp <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe77010>\n",
      "prospecting_level <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe77040>\n",
      "prospecting_exp <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe77070>\n",
      "carving_level <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe770a0>\n",
      "carving_exp <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe770d0>\n",
      "alchemy_level <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe77100>\n",
      "alchemy_exp <nmmo.datastore.serialized.SerializedAttribute object at 0x7efe1fe77130>\n",
      "realm <nmmo.core.realm.Realm object at 0x7efe573fed10>\n",
      "config <nmmo.core.config.Default object at 0x7efe2cd93eb0>\n",
      "_np_random RandomNumberGenerator(PCG64)\n",
      "policy Neural\n",
      "entity_id 1\n",
      "repr None\n",
      "name Neural1\n",
      "vision 7\n",
      "attacker None\n",
      "target None\n",
      "closest None\n",
      "spawn_pos (48, 144)\n",
      "status <nmmo.entity.entity.Status object at 0x7efe1fe77160>\n",
      "history <nmmo.entity.entity.History object at 0x7efe1fe771c0>\n",
      "resources <nmmo.entity.entity.Resources object at 0x7efe1fe771f0>\n",
      "inventory <nmmo.systems.inventory.Inventory object at 0x7efe1fe77220>\n",
      "agent <nmmo.core.agent.Agent object at 0x7efe220d8580>\n",
      "immortal False\n",
      "buys 0\n",
      "sells 0\n",
      "ration_consumed 0\n",
      "poultice_consumed 0\n",
      "ration_level_consumed 0\n",
      "poultice_level_consumed 0\n",
      "skills <nmmo.systems.skill.Skills object at 0x7efe1fe774f0>\n"
     ]
    }
   ],
   "source": [
    "for key, value in env.realm.players.items():\n",
    "    print(key, value.__dict__.keys())\n",
    "    for key2, value2 in value.__dict__.items():\n",
    "        print(key2, value2)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 12, 81])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming x is your tensor\n",
    "x = torch.rand(768, 16, 9, 9)\n",
    "\n",
    "# Reshape the tensor to flatten the spatial dimensions\n",
    "x = x.view(768, 16, -1)\n",
    "#print(x.shape)  # Should print torch.Size([768, 4, 81])\n",
    "from reinforcement_learning.model_util import TransformerBlock\n",
    "# RuntimeError: shape '[768, -1, 3, 4, 20]' is invalid for input of size 2985984\n",
    "l = TransformerBlock(dim=81)\n",
    "x= l(x)\n",
    "print(x.shape)  # Should print torch.Size([768, 81, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Policy values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy(\n",
      "  (policy): Baseline(\n",
      "    (tile_encoder): TileEncoder(\n",
      "      (embedding): Embedding(768, 32)\n",
      "      (tile_conv_1): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (tile_conv_2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (tile_fc): Linear(in_features=968, out_features=256, bias=True)\n",
      "    )\n",
      "    (player_encoder): PlayerEncoder(\n",
      "      (embedding): Embedding(7936, 32)\n",
      "      (agent_fc): Linear(in_features=992, out_features=256, bias=True)\n",
      "      (my_agent_fc): Linear(in_features=992, out_features=256, bias=True)\n",
      "    )\n",
      "    (item_encoder): ItemEncoder(\n",
      "      (embedding): Embedding(256, 32)\n",
      "      (fc): Linear(in_features=76, out_features=256, bias=True)\n",
      "    )\n",
      "    (inventory_encoder): InventoryEncoder(\n",
      "      (fc): Linear(in_features=3072, out_features=256, bias=True)\n",
      "    )\n",
      "    (market_encoder): MarketEncoder(\n",
      "      (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (task_encoder): TaskEncoder(\n",
      "      (fc): Linear(in_features=4096, out_features=256, bias=True)\n",
      "    )\n",
      "    (proj_fc): Linear(in_features=1280, out_features=256, bias=True)\n",
      "    (action_decoder): ActionDecoder(\n",
      "      (layers): ModuleDict(\n",
      "        (attack_style): Linear(in_features=256, out_features=3, bias=True)\n",
      "        (attack_target): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (market_buy): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (inventory_destroy): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (inventory_give_item): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (inventory_give_player): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (gold_quantity): Linear(in_features=256, out_features=99, bias=True)\n",
      "        (gold_target): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (move): Linear(in_features=256, out_features=5, bias=True)\n",
      "        (inventory_sell): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (inventory_price): Linear(in_features=256, out_features=99, bias=True)\n",
      "        (inventory_use): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (value_head): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# I want to see what is inside the pytorch model/home/saidinesh/Desktop/Projects/baselines/starter-kit-nmmo/my-submission/nmmo_20231108_125354.000305.pt\n",
    "import torch\n",
    "model = torch.load('/home/saidinesh/Desktop/Projects/baselines/starter-kit-nmmo/my-submission/nmmo_20231108_125354.000305.pt')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 15), dtype=float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nmmo.lib.log import EventCode\n",
    "env = nmmo.Env()\n",
    "log = env.realm.event_log.get_data( #agents=[1],\n",
    "                                        event_code=EventCode.PLAYER_KILL,\n",
    "                                        tick=env.realm.tick)\n",
    "log                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 67.7%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def get_memory_usage():\n",
    "    memory = psutil.virtual_memory()\n",
    "    return memory.percent\n",
    "\n",
    "print(f\"Memory usage: {get_memory_usage()}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curriculum exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaskSpec(eval_fn=<function CountEvent at 0x7f70c1e57370>, eval_fn_kwargs={'event': 'GO_FARTHEST', 'N': 10}, task_cls=<class 'nmmo.task.task_api.Task'>, task_kwargs={}, reward_to='agent', sampling_weight=1.0, embedding=array([-0.3457 ,  0.2172 , -0.1261 , ..., -0.09106,  0.1076 , -0.1691 ],\n",
      "      dtype=float16), predicate=None), TaskSpec(eval_fn=<function CountEvent at 0x7f70c1e57370>, eval_fn_kwargs={'event': 'EAT_FOOD', 'N': 10}, task_cls=<class 'nmmo.task.task_api.Task'>, task_kwargs={}, reward_to='agent', sampling_weight=1.0, embedding=array([-0.3423 ,  0.1643 , -0.2136 , ..., -0.09735,  0.05557, -0.12024],\n",
      "      dtype=float16), predicate=None), TaskSpec(eval_fn=<function CountEvent at 0x7f70c1e57370>, eval_fn_kwargs={'event': 'DRINK_WATER', 'N': 10}, task_cls=<class 'nmmo.task.task_api.Task'>, task_kwargs={}, reward_to='agent', sampling_weight=1.0, embedding=array([-0.3499 ,  0.2294 , -0.06683, ..., -0.1581 ,  0.01814, -0.1663 ],\n",
      "      dtype=float16), predicate=None), TaskSpec(eval_fn=<function CountEvent at 0x7f70c1e57370>, eval_fn_kwargs={'event': 'SCORE_HIT', 'N': 10}, task_cls=<class 'nmmo.task.task_api.Task'>, task_kwargs={}, reward_to='agent', sampling_weight=1.0, embedding=array([-0.4011 ,  0.1316 , -0.08136, ..., -0.0853 ,  0.0881 , -0.0452 ],\n",
      "      dtype=float16), predicate=None), TaskSpec(eval_fn=<function CountEvent at 0x7f70c1e57370>, eval_fn_kwargs={'event': 'HARVEST_ITEM', 'N': 10}, task_cls=<class 'nmmo.task.task_api.Task'>, task_kwargs={}, reward_to='agent', sampling_weight=1.0, embedding=array([-0.32   ,  0.1405 , -0.2069 , ..., -0.1027 ,  0.08624, -0.0997 ],\n",
      "      dtype=float16), predicate=None), TaskSpec(eval_fn=<function CountEvent at 0x7f70c1e57370>, eval_fn_kwargs={'event': 'LEVEL_UP', 'N': 10}, task_cls=<class 'nmmo.task.task_api.Task'>, task_kwargs={}, reward_to='agent', sampling_weight=1.0, embedding=array([-0.3787 ,  0.162  , -0.1025 , ..., -0.0501 ,  0.0801 , -0.06995],\n",
      "      dtype=float16), predicate=None), TaskSpec(eval_fn=<function PracticeEating at 0x7f70f6fda320>, eval_fn_kwargs={}, task_cls=<class 'nmmo.task.task_api.Task'>, task_kwargs={}, reward_to='agent', sampling_weight=1.0, embedding=array([-0.0937,  0.2603, -0.1652, ..., -0.2456,  0.1525, -0.573 ],\n",
      "      dtype=float16), predicate=None), TaskSpec(eval_fn=<function PracticeInventoryManagement at 0x7f70f6fda440>, eval_fn_kwargs={'space': 2, 'num_tick': 500}, task_cls=<class 'nmmo.task.task_api.Task'>, task_kwargs={}, reward_to='agent', sampling_weight=1.0, embedding=array([-0.05872,  0.02336, -0.0881 , ..., -0.2476 , -0.1294 , -0.354  ],\n",
      "      dtype=float16), predicate=None), TaskSpec(eval_fn=<function PracticeInventoryManagement at 0x7f70f6fda440>, eval_fn_kwargs={'space': 4, 'num_tick': 500}, task_cls=<class 'nmmo.task.task_api.Task'>, task_kwargs={}, reward_to='agent', sampling_weight=1.0, embedding=array([-0.03014,  0.01453, -0.08466, ..., -0.2468 , -0.11676, -0.3472 ],\n",
      "      dtype=float16), predicate=None), TaskSpec(eval_fn=<function PracticeInventoryManagement at 0x7f70f6fda440>, eval_fn_kwargs={'space': 8, 'num_tick': 500}, task_cls=<class 'nmmo.task.task_api.Task'>, task_kwargs={}, reward_to='agent', sampling_weight=1.0, embedding=array([-0.02946,  0.0358 , -0.0945 , ..., -0.2632 , -0.11975, -0.3508 ],\n",
      "      dtype=float16), predicate=None)]\n"
     ]
    }
   ],
   "source": [
    "# /home/saidinesh/Desktop/Projects/baselines/reinforcement_learning/eval_task_with_embedding.pkl\n",
    "# read that file and print keys\n",
    "import dill\n",
    "with open('/home/saidinesh/Desktop/Projects/baselines/curriculum_generation/custom_curriculum_with_embedding.pkl', 'rb') as f:\n",
    "    data = dill.load(f)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the data list and print the keys\n",
    "# each element is TaskSpec(eval_fn=<function CountEvent at 0x7fecb8f67520>, eval_fn_kwargs={'event': 'EAT_FOOD', 'N': 10}, task_cls=<class 'nmmo.task.task_api.Task'>, task_kwargs={}, reward_to='agent', sampling_weight=1.0, embedding=array([-0.3462,  0.1978, -0.2805, ..., -0.1089,  0.0671, -0.0725],\n",
    "# now create dataframe with data eval_fn_kwargs, task_cls, task_kwargs,\n",
    "import pandas as pd\n",
    "# create empty dataframe\n",
    "df = pd.DataFrame(columns=['eval_fn_kwargs',  ])\n",
    "# iterate through the data list and append the data to the dataframe\n",
    "#AttributeError: 'DataFrame' object has no attribute 'append'\n",
    "for i in range(len(data)):\n",
    "    print(data[i].eval_fn_kwargs, ) # {'num_tick': 1024} <class 'nmmo.task.task_api.Task'> \n",
    "    df = df._append({'eval_fn_kwargs': data[i].eval_fn_kwargs,  },ignore_index=True)\n",
    "df.to_csv('reinforcement_learning/curriculum_task_with_embedding.csv')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "\n",
    "def create_custom_policy_pt(policy_file, pth_file, out_name=\"my_submission.pkl\"):\n",
    "  assert out_name.endswith(\".pkl\"), \"The file name must end with .pkl\"\n",
    "  with open(policy_file, \"r\") as f:\n",
    "    src_code = f.read()\n",
    "\n",
    "  # add the make_policy() function\n",
    "  # YOU SHOULD CHECK the name of your policy (if not Baseline),\n",
    "  # and the args that go into the policy\n",
    "  src_code += \"\"\"\n",
    "\n",
    "class Config:\n",
    "    # Run a smaller config on your local machine\n",
    "    local_mode = False  # Run in local mode\n",
    "    # Track to run - options: reinforcement_learning, curriculum_generation\n",
    "    track = \"rl\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # record_loss = False  # log all minibatch loss and actions, for debugging\n",
    "\n",
    "    # Trainer Args\n",
    "    seed = 9\n",
    "    num_cores = None  # Number of cores to use for training\n",
    "    num_envs = 6  # Number of environments to use for training\n",
    "    num_buffers = 2  # Number of buffers to use for training\n",
    "    rollout_batch_size = 2**15  # Number of steps to rollout\n",
    "    eval_batch_size = 2**15  # Number of steps to rollout for eval\n",
    "    train_num_steps = 10_000_000  # 10_000_000  # Number of steps to train\n",
    "    eval_num_steps = 1_000_000  # 1_000_000  # Number of steps to evaluate\n",
    "    checkpoint_interval = 5_000_000  # Interval to save models\n",
    "    # f\"nmmo_{time.strftime('%Y%m%d_%H%M%S')}_{seed}\"  # Run name\n",
    "    run_name = f\"nmmo_rp_cr_attn_lstm_seed{seed}_exp17\"\n",
    "    runs_dir = \"./runs\"  # Directory for runs\n",
    "    policy_store_dir = None  # Policy store directory\n",
    "    use_serial_vecenv = False  # Use serial vecenv implementation\n",
    "    learner_weight = 1.0  # Weight of learner policy\n",
    "    max_opponent_policies = 0  # Maximum number of opponent policies to train against\n",
    "    eval_num_policies = 2  # Number of policies to use for evaluation\n",
    "    eval_num_rounds = 1  # Number of rounds to use for evaluation\n",
    "    wandb_project = 'nmo_baseline_ppo'  # WandB project name\n",
    "    wandb_entity = 'saidineshpola'  # WandB entity name\n",
    "\n",
    "    # PPO Args\n",
    "    # Train on this number of steps of a rollout at a time. Used to reduce GPU memory.\n",
    "    bptt_horizon = 8\n",
    "    ppo_training_batch_size = 128  # Number of rows in a training batch\n",
    "    ppo_update_epochs = 3  # Number of update epochs to use for training\n",
    "    ppo_learning_rate = 0.00015  # Learning rate\n",
    "    clip_coef = 0.1  # PPO clip coefficient\n",
    "\n",
    "    # Environment Args\n",
    "    num_agents = 128  # Number of agents to use for training\n",
    "    num_npcs = 256  # Number of NPCs to use for training\n",
    "    max_episode_length = 1024  # Number of steps per episode\n",
    "    death_fog_tick = None  # Number of ticks before death fog starts\n",
    "    num_maps = 128  # Number of maps to use for training\n",
    "    maps_path = \"maps/train/\"  # Path to maps to use for training\n",
    "    map_size = 128  # Size of maps to use for training\n",
    "    # Percentage of agents to be resilient to starvation/dehydration\n",
    "    resilient_population = 0.2\n",
    "    tasks_path = None  # Path to tasks to use for training\n",
    "    eval_mode = False  # Run the postprocessor in the eval mode\n",
    "    # Stop the episode when the number of agents reaches this number\n",
    "    early_stop_agent_num = 8\n",
    "    sqrt_achievement_rewards = False  # Use the log of achievement rewards\n",
    "    heal_bonus_weight = 0.03\n",
    "    meander_bonus_weight = 0.02\n",
    "    explore_bonus_weight = 0.01\n",
    "    gold_bonus_weight = 0  # 0.002\n",
    "    attack_bonus_weight = 0  # 0.03 added\n",
    "    spawn_immunity = 20\n",
    "\n",
    "    # Policy Args\n",
    "    input_size = 256\n",
    "    hidden_size = 256\n",
    "    num_lstm_layers = 5  # Number of LSTM layers to use\n",
    "    task_size = 4096  # Size of task embedding\n",
    "    encode_task = True  # Encode task\n",
    "    attend_task = \"none\"  # Attend task - options: none, pytorch, nikhil\n",
    "    attentional_decode = True  # Use attentional action decoder\n",
    "    extra_encoders = True  # Use inventory and market encoders\n",
    "\n",
    "    @classmethod\n",
    "    def asdict(cls):\n",
    "        return {attr: getattr(cls, attr) for attr in dir(cls)\n",
    "                if not callable(getattr(cls, attr)) and not attr.startswith(\"__\")}\n",
    "\n",
    "\n",
    "def make_policy(envs):\n",
    "      from pufferlib.frameworks import cleanrl\n",
    "      env = pufferlib.emulation.PettingZooPufferEnv(nmmo.Env(Config()))\n",
    "        learner_policy = BaselineNew(\n",
    "            env,\n",
    "            input_size=256,\n",
    "            hidden_size=256,\n",
    "            task_size=4096\n",
    "        )\n",
    "        return cleanrl.Policy(learner_policy)\n",
    "  \"\"\"\n",
    "  state_dict = torch.load(pth_file)\n",
    "  checkpoint = {\n",
    "    \"policy_src\": src_code,\n",
    "    \"state_dict\": state_dict,\n",
    "  }\n",
    "  with open(out_name, \"wb\") as out_file:\n",
    "    pickle.dump(checkpoint, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_policy_file='reinforcement_learning/rl_policy.py'\n",
    "checkpoint_to_submit='runs/nmmo_rp_cr_attn_lstm_seed9_exp17/policy_store/nmmo_rp_cr_attn_lstm_seed9_exp17.000305.pt'\n",
    "create_custom_policy_pt(custom_policy_file, checkpoint_to_submit,\n",
    "                        out_name=\"/home/saidinesh/Desktop/Projects/baselines/starter-kit-nmmo/my-submission/lstm_submission.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
